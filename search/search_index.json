{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Biodata Penulis Nama : IKROMUL ISLAM N.I.M : 170441100090 Program Studi : Sistem Informasi Universitas : Univ. Trunojoyo Madura","title":"Biografi"},{"location":"#biodata-penulis","text":"Nama : IKROMUL ISLAM N.I.M : 170441100090 Program Studi : Sistem Informasi Universitas : Univ. Trunojoyo Madura","title":"Biodata Penulis"},{"location":"KNN/","text":"Teknik Data Mining : Algoritma K-Nearest Neighbor Apa itu K-Nearest Neighbor (KNN) Algorithm? \u2022 K-nearest neighbor adalah algoritma supervised learning dimana hasil dari instance yang baru diklasifikasikan berdasarkan mayoritas darikategori K-tetangga terdekat. \u2022 Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan sampel2 dari data training. \u2022 Algoritma K Nearest neighbor menggunakan neighborhood classification sebagai nilai prediksi dari nilai instance yang baru. Berikut ini langkah-langkah dari algoritma Knearest neighbors (KNN): Tentukan parameter K = jumlah banyaknya tetangga terdekat Hitung jarak antara data baru dan semua data yang ada di data training. Urutkan jarak tersebut dan tentukan tetangga mana yang terdekat berdasarkan jarak minimum ke-K. Tentukan kategori dari tetangga terdekat. Gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi dari data yang baru. Contoh program KNN seperti dibawah ini : import numpy as np from sklearn import neighbors , datasets from sklearn import preprocessing n_neighbors = 20 # import data iris iris = datasets . load_iris () # menyiapkan data X = iris . data [:, : 150 ] y = iris . target # membuat instance dari KNN dan menyesuaikan data clf = neighbors . KNeighborsClassifier ( n_neighbors , weights = distance ) clf . fit ( X , y ) # membuat prediksi kelas sl = input ( Enter sepal length (cm): ) sw = input ( Enter sepal width (cm): ) pl = input ( Enter petal length (cm): ) pw = input ( Enter petal width (cm): ) dataClass = clf . predict ([[ sl , sw , pl , pw ]]) print ( Prediction: ), if dataClass == 0 : print ( Iris Setosa ) elif dataClass == 1 : print ( Iris Versicolour ) else : print ( Iris Virginica ) Daftar Pustaka http://depandienda.it.student.pens.ac.id/file/knn_references.pdf","title":"K-nearest neighbour"},{"location":"KNN/#teknik-data-mining-algoritma-k-nearest-neighbor","text":"Apa itu K-Nearest Neighbor (KNN) Algorithm? \u2022 K-nearest neighbor adalah algoritma supervised learning dimana hasil dari instance yang baru diklasifikasikan berdasarkan mayoritas darikategori K-tetangga terdekat. \u2022 Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan sampel2 dari data training. \u2022 Algoritma K Nearest neighbor menggunakan neighborhood classification sebagai nilai prediksi dari nilai instance yang baru. Berikut ini langkah-langkah dari algoritma Knearest neighbors (KNN): Tentukan parameter K = jumlah banyaknya tetangga terdekat Hitung jarak antara data baru dan semua data yang ada di data training. Urutkan jarak tersebut dan tentukan tetangga mana yang terdekat berdasarkan jarak minimum ke-K. Tentukan kategori dari tetangga terdekat. Gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi dari data yang baru. Contoh program KNN seperti dibawah ini : import numpy as np from sklearn import neighbors , datasets from sklearn import preprocessing n_neighbors = 20 # import data iris iris = datasets . load_iris () # menyiapkan data X = iris . data [:, : 150 ] y = iris . target # membuat instance dari KNN dan menyesuaikan data clf = neighbors . KNeighborsClassifier ( n_neighbors , weights = distance ) clf . fit ( X , y ) # membuat prediksi kelas sl = input ( Enter sepal length (cm): ) sw = input ( Enter sepal width (cm): ) pl = input ( Enter petal length (cm): ) pw = input ( Enter petal width (cm): ) dataClass = clf . predict ([[ sl , sw , pl , pw ]]) print ( Prediction: ), if dataClass == 0 : print ( Iris Setosa ) elif dataClass == 1 : print ( Iris Versicolour ) else : print ( Iris Virginica )","title":"Teknik Data Mining : Algoritma K-Nearest Neighbor"},{"location":"KNN/#daftar-pustaka","text":"http://depandienda.it.student.pens.ac.id/file/knn_references.pdf","title":"Daftar Pustaka"},{"location":"Kmeans/","text":"Teknik Data Mining : Algoritma K-Means Clustering K-means clustering merupakan salah satu metode cluster analysis non hirarki yang berusaha untuk mempartisi objek yang ada kedalam satu atau lebih cluster atau kelompok objek berdasarkan karakteristiknya, sehingga objek yang mempunyai karakteristik yang sama dikelompokan dalam satu cluster yang sama dan objek yang mempunyai karakteristik yang berbeda dikelompokan kedalam cluster yang lain. Menurut Daniel dan Eko, Langkah-langkah algoritma K-Means adalah sebagai berikut: a. Pilih secara acak k buah data sebagai pusat cluster. b. Jarak antara data dan pusat cluster dihitung menggunakan Euclidian Distance. Untuk menghitung jarak semua data ke setiap titik pusat cluster dapat menggunakan teori jarak Euclidean yang dirumuskan sebagai berikut: dimana: d(x,y) = Jarak objek antara objek x dan j n = Dimensi data Xi = nilai (angka) pada kolom yang dijadikan centorid yi = nilai (angka) pada kolom yang akan diuji c. Data ditempatkan dalam cluster yang terdekat, dihitung dari tengah cluster. d. Pusat cluster baru akan ditentukan bila semua data telah ditetapkan dalam cluster terdekat. e. Proses penentuan pusat cluster dan penempatan data dalam cluster diulangi sampai nilai centroid tidak berubah lagi. Berikut ini adalah contoh penerapan algoritma K-Means : Pertama kita tentukan centroidnya, yaitu baris B, E, F. Kemudian kita hitung jarak dari setiap data terhadap centroidnya. Dan diperoleh hasil seperti dibawah ini. Kemudian kita tentukan centroid baru dengan cara mengitung rata-rata dari data-data pada setiap cluster. Selanjutnya kita lakukan iterasi clustering kedua untuk melihat apakah data-data ada yang berpindah cluster Setelah dilakukan perhitungan clustering kedua, ternyata cluster dari data-data masih tetap seperti perhitungan clustering pertama. Jadi perhitungan clustering berhenti. Daftar Pustaka [1.]Baradwaj, B. K. and Pal, S. (2011). \u201cMining Educational Data to Analyze Student\u2019s Performance.\u201d International Journal of Advanced Computer Science and Applications. 2. 64. [2.] Begum, S. H. (2013). \u201cData Mining Tools and Trends - An Overview.\u201d International Journal of Emerging Research in Management Technology. 2278-9359. 6. [3.] Daniel Riano Kaparang dan Eko Sediyono. (2013). \u201cPenentuan Alih Fungsi Lahan Marginal Menjadi Lahan Pangan Berbasis Algoritma K-means di Wilayah Kabupaten Boyolali.\u201d JdC. 2. 20. [4.] Deka Dwinavinta Candra Nugraha, Zumrotun Naimah, Makhfuzi Fahmi dan Novi Setiani. (2014). \u201cKlasterisasi Judul Buku dengan Menggunakan Metode K-Means.\u201d Seminar Nasional Aplikasi Teknologi Informasi. ISSN: 1907-5022. G-2. [5.] Ediyanto, Muhlasah Novitasari Mara dan Neva Satyahadewi. (2013). \u201cPengklasifikasian Karakteristik dengan Metode K-means Cluster Analysis.\u201d Buletin Ilmiah Mat. Stat. dan Terapannya (Bilmaster). 2. 134. [6.] Johan Oscar Ong. (2013). \u201cImplementasi Algoritma K-means Clustering untuk Menentukan Strategi Marketing President University.\u201d Jurnal Ilmiah Teknik Industri. 12. 13-20. [7.] Mujib Ridwan, Hadi Suyono dan M. Sarosa. (2013). \u201cPenerapan Data Mining untuk Evaluasi Kinerja Akademik Mahasiswa Menggunakan Algoritma Naive Bayes Classifier.\u201d Jurnal EECCIS. 7. 60-61. [8.] Oyelade, O. J., Oladipupo, O. O. and Obagbuwa, I. C. (2010). \u201cApplication of K-Means Clustering algorithm for Prediction of Student\u2019s Academic Performance.\u201d International Journal of Computer Science and Information Scurity. 7. 292. [9.] Seddawy, A. B. E., Khedr, A. and Sultan, T. (2012). \u201cAdapted Framework for Data Mining Technique to Improve Decision Support System in an Uncertain Situation.\u201d International Journal of Data Mining Kenowledge Management Process. 2. 5. [10.] Sudirman dan Nur Ani. (2012). \u201cImplementasi Teknik Data Mining Dengan Algoritma Kmeans Clustering dan Fungsi Kernel Polynominal untuk Klasterisasi Objek Data.\u201d Prosiding Seminar Nasional Efisiensi Energi untuk Peningkatan Daya Saing Industri Manufaktur Otomotif Nasional. B - 50. [11.] Yedla, M., Pathakota, S. R. and Srinivasa, T. M. (2010). \u201cEnhancing K-means Clustering Algorithm with Improved Initial Center.\u201d International Journal of Computer Science and Information Technologies. 1. 121.","title":"K-means"},{"location":"Kmeans/#teknik-data-mining-algoritma-k-means-clustering","text":"K-means clustering merupakan salah satu metode cluster analysis non hirarki yang berusaha untuk mempartisi objek yang ada kedalam satu atau lebih cluster atau kelompok objek berdasarkan karakteristiknya, sehingga objek yang mempunyai karakteristik yang sama dikelompokan dalam satu cluster yang sama dan objek yang mempunyai karakteristik yang berbeda dikelompokan kedalam cluster yang lain. Menurut Daniel dan Eko, Langkah-langkah algoritma K-Means adalah sebagai berikut: a. Pilih secara acak k buah data sebagai pusat cluster. b. Jarak antara data dan pusat cluster dihitung menggunakan Euclidian Distance. Untuk menghitung jarak semua data ke setiap titik pusat cluster dapat menggunakan teori jarak Euclidean yang dirumuskan sebagai berikut: dimana: d(x,y) = Jarak objek antara objek x dan j n = Dimensi data Xi = nilai (angka) pada kolom yang dijadikan centorid yi = nilai (angka) pada kolom yang akan diuji c. Data ditempatkan dalam cluster yang terdekat, dihitung dari tengah cluster. d. Pusat cluster baru akan ditentukan bila semua data telah ditetapkan dalam cluster terdekat. e. Proses penentuan pusat cluster dan penempatan data dalam cluster diulangi sampai nilai centroid tidak berubah lagi. Berikut ini adalah contoh penerapan algoritma K-Means : Pertama kita tentukan centroidnya, yaitu baris B, E, F. Kemudian kita hitung jarak dari setiap data terhadap centroidnya. Dan diperoleh hasil seperti dibawah ini. Kemudian kita tentukan centroid baru dengan cara mengitung rata-rata dari data-data pada setiap cluster. Selanjutnya kita lakukan iterasi clustering kedua untuk melihat apakah data-data ada yang berpindah cluster Setelah dilakukan perhitungan clustering kedua, ternyata cluster dari data-data masih tetap seperti perhitungan clustering pertama. Jadi perhitungan clustering berhenti.","title":"Teknik Data Mining : Algoritma K-Means Clustering"},{"location":"Kmeans/#daftar-pustaka","text":"[1.]Baradwaj, B. K. and Pal, S. (2011). \u201cMining Educational Data to Analyze Student\u2019s Performance.\u201d International Journal of Advanced Computer Science and Applications. 2. 64. [2.] Begum, S. H. (2013). \u201cData Mining Tools and Trends - An Overview.\u201d International Journal of Emerging Research in Management Technology. 2278-9359. 6. [3.] Daniel Riano Kaparang dan Eko Sediyono. (2013). \u201cPenentuan Alih Fungsi Lahan Marginal Menjadi Lahan Pangan Berbasis Algoritma K-means di Wilayah Kabupaten Boyolali.\u201d JdC. 2. 20. [4.] Deka Dwinavinta Candra Nugraha, Zumrotun Naimah, Makhfuzi Fahmi dan Novi Setiani. (2014). \u201cKlasterisasi Judul Buku dengan Menggunakan Metode K-Means.\u201d Seminar Nasional Aplikasi Teknologi Informasi. ISSN: 1907-5022. G-2. [5.] Ediyanto, Muhlasah Novitasari Mara dan Neva Satyahadewi. (2013). \u201cPengklasifikasian Karakteristik dengan Metode K-means Cluster Analysis.\u201d Buletin Ilmiah Mat. Stat. dan Terapannya (Bilmaster). 2. 134. [6.] Johan Oscar Ong. (2013). \u201cImplementasi Algoritma K-means Clustering untuk Menentukan Strategi Marketing President University.\u201d Jurnal Ilmiah Teknik Industri. 12. 13-20. [7.] Mujib Ridwan, Hadi Suyono dan M. Sarosa. (2013). \u201cPenerapan Data Mining untuk Evaluasi Kinerja Akademik Mahasiswa Menggunakan Algoritma Naive Bayes Classifier.\u201d Jurnal EECCIS. 7. 60-61. [8.] Oyelade, O. J., Oladipupo, O. O. and Obagbuwa, I. C. (2010). \u201cApplication of K-Means Clustering algorithm for Prediction of Student\u2019s Academic Performance.\u201d International Journal of Computer Science and Information Scurity. 7. 292. [9.] Seddawy, A. B. E., Khedr, A. and Sultan, T. (2012). \u201cAdapted Framework for Data Mining Technique to Improve Decision Support System in an Uncertain Situation.\u201d International Journal of Data Mining Kenowledge Management Process. 2. 5. [10.] Sudirman dan Nur Ani. (2012). \u201cImplementasi Teknik Data Mining Dengan Algoritma Kmeans Clustering dan Fungsi Kernel Polynominal untuk Klasterisasi Objek Data.\u201d Prosiding Seminar Nasional Efisiensi Energi untuk Peningkatan Daya Saing Industri Manufaktur Otomotif Nasional. B - 50. [11.] Yedla, M., Pathakota, S. R. and Srinivasa, T. M. (2010). \u201cEnhancing K-means Clustering Algorithm with Improved Initial Center.\u201d International Journal of Computer Science and Information Technologies. 1. 121.","title":"Daftar Pustaka"},{"location":"Tree/","text":"Decision Tree Classifier (Gain) \u200b Secara singkat bahwa Decision Tree merupakan salah satu metode klasifikasi pada Text Mining. Klasifikasi adalah proses menemukan kumpulan pola atau fungsi-fungsi yang mendeskripsikan dan memisahkan kelas data satu dengan lainnya, untuk dapat digunakan untuk memprediksi data yang belum memiliki kelas data tertentu (Jianwei Han, 2001). \u200b Decision Tree adalah sebuah struktur pohon, dimana setiap node pohon merepresentasikan atribut yang telah diuji, setiap cabang merupakan suatu pembagian hasil uji, dan node daun (leaf) merepresentasikan kelompok kelas tertentu. Level node teratas dari sebuah Decision Tree adalah node akar (root) yang biasanya berupa atribut yang paling memiliki pengaruh terbesar pada suatu kelas tertentu. Pada umumnya Decision Tree melakukan strategi pencarian secara top-down untuk solusinya. Pada proses mengklasifikasi data yang tidak diketahui, nilai atribut akan diuji dengan cara melacak jalur dari node akar (root) sampai node akhir (daun) dan kemudian akan diprediksi kelas yang dimiliki oleh suatu data baru tertentu. \u200b Kelebihan lain dari metode ini adalah mampu mengeliminasi perhitungan atau data-data yang kiranya tidak diperlukan. Sebab, sampel yang ada biasanya hanya diuji berdasarkan kriteria atau kelas tertentu saja. \u200b Meski memiliki banyak kelebihan, namun bukan berarti metode ini tidak memiliki kekurangan. Decision tree ini bisa terjadi overlap, terutama ketika kelas dan kriteria yang digunakan sangat banyak tentu saja dapat meningkatkan waktu pengambilan keputusan sesuai dengan jumlah memori yang dibutuhkan. \u200b Dalam hal akumulasi, decision tree juga seringkali mengalami kendala eror terutama dalam jumlah besar. Selain itu, terdapat pula kesulitan dalam mendesain decision tree yang optimal. Apalagi mengingat kualitas keputusan yang didapatkan dari metode decision tree sangat tergantung pada bagaimana pohon tersebut didesain. \u200b Terlepas dari kekurangan dan kelebihan dari decision tree , metode ini banyak digunakan lebih lanjut dalam berbagai pengolahan data. Mulai dari data mining dan juga machine learning . Dalam dunia kerja, decision tree sendiri sangat berguna untuk penilaian credit scoring. Jika anda pernah mengajukan kredit yang diproses secara instan, nah anda sudah mempunyai pengalaman dari decision tree . Struktur dan Algoritma Dasar Pembelajaran Pohon Keputusan Pada pohon keputusan terdapat tiga jenis node , antara lain : 1. Akar Merupakan node teratas, pada node ini tidak ada input dan dapat tidak mempunyai output atau dapat mempunyai output lebih dari satu. 2. Internal node Merupakan node percabangan, pada node ini hanya terdapat satu input dan mempunyai output minimal dua. 3. Daun Merupakan node akhir atau terminal node , pada node ini hanya terdapat satu input dan tidak mempunyai output (simpul terminal). Kebanyakan algoritma untuk pembelajaran pohon keputusan adalah variasi dari algoritma intinya yang menggunakan pencarian rakus ( greedy ) dari atas ke bawah terhadap ruang kemungkinan pohon keputusan. Salah satu algoritmanya yaitu ID3. Langkah-langkah yang dilakukan yaitu, Tentukan atribut terbaik untuk root dari tree . Setiap instan dari atribut dievaluasi menggunakan sebuah tes statistik untuk menentukan seberapa bagus atribut tersebut mengklasifikasi sampel latihan. Turunan dari node root dibuat untuk setiap kemungkinan nilai dari atribut Sampel latihan kemudian diurutkan berdasarkan node turunan. Seluruh proses kemudian diulang menggunakan sampel latihan yang berhubungan dengan setiap node turunan untuk menentukan atribut terbaik untuk dites di pohon. Entropy Information Gain Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur \u201cseberapa informatifnya\u201d sebuah node (yang biasanya disebut seberapa baiknya). Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama. Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama. 0 Entropi(S) 1, jika jumlah contoh positif dan negatif dalam S tidak sama. Dimana: \u2022 S adalah himpunan (dataset) kasus \u2022 k adalah banyaknya partisi S \u2022 pj adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus. Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar. Implementasi Program Menggunakan Perhitungan Entropy Import Library import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn import model_selection from sklearn import metrics from sklearn.model_selection import train_test_split from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO import pydotplus from IPython.display import Image Load dataset #memuat file excel df=pd.read_excel( Cryotherapy.xlsx ) #menampilkan data print( Informasi Data\\n ) print( Jumlah Data : , len(df)) print ( Dimensi Data : ,df.shape) print ( Dataset : ) print(df.head()) print( \\n ) Memilah data jika data sudah sukses terimport, selanjutnya kita akan memilah data ke beberapa bagian seperti data yang akan d jadikan testing, training, dan yang akan dijadikan classnya. di sini saya menggunakan 15 data untuk testingnya dan data kolom terakhir sebagai classnya. berikut kodenya : #splitting dataset ke training dan testing train, test = train_test_split(df, test_size = 0.1,random_state=1234) #mencari hasil print(train.shape) print(test.shape) # Dataset validasi dataset array = df.values X = array[:,1:6] Y = array[:,6] validation_size = 15 seed = 7 X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed) Tampil Hasil Pemilahan Data #mencari hasil print(X_train.shape) print(Y_train.shape) print(X_validation.shape) print(Y_validation.shape) Prediksi Data Menggunakan Library scikit lear metode Decision Tree Entropy entropy= DecisionTreeClassifier(criterion= entropy ,random_state=1234) #learning entropy.fit(X_train,Y_train) #Prediksi prediction=entropy.predict(X_validation) #mengevaluasi(Accuracy) print( Accuracy: ,metrics.accuracy_score(prediction,Y_validation)) #evaluation(Confusion Metrix) print( Confusion Metrix:\\n ,metrics.confusion_matrix(prediction,Y_validation)) Menampilkan Pohon Data feature_cols=[ age , Time , Number_of_Warts , Type , Area ] dot_data = StringIO() export_graphviz(entropy, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=[ Positive , negative ]) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png( entropy.png ) Image(graph.create_png()) Menampilkan Hasil Prediksi Data Entropy print( Hasil prediksi menggunakan entropy ) #Membentuk kembali diperlukan untuk melakukan penggabungan pred_clf_df = pd.DataFrame(prediction.reshape(15,1)) #Ganti nama kolom untuk menunjukkan prediksi pred_clf_df.rename(columns={0: Prediction }, inplace=True) #membentuk kembali dataset uji X_validation_df = pd.DataFrame(X_validation.reshape(15 ,5)) #menggabungkan dua bingkai data panda di atas kolom untuk membuat dataset prediksi pred_outcome = pd.concat([X_validation_df, pred_clf_df], axis=1, join_axes=[X_validation_df.index]) pred_outcome.rename(columns = {0: age ,1: Time ,2: Number_of_Warts ,3: Type ,4: Area }, inplace=True) #cetak 10 baris prediksi akhir print((pred_outcome).head(15)) print ( \\n ) #mengevaluasi(Accuracy) print( Accuracy: ,metrics.accuracy_score(prediction,Y_validation)) Prediksi Data Menggunakan Library scikit learn metode Decision Tree Gini Kemudian kita coba menggunakan metode perhitungan gini. Adapun caranya sama seperti decision tree. yang membedakannya adalah criterionnya. kita tinggal mengubahnya menjadi gini. gini= DecisionTreeClassifier(criterion= gini ,random_state=1234) #learning gini.fit(X_train,Y_train) #Prediksi prediction_gini=gini.predict(X_validation) #mengevaluasi(Accuracy) print( Accuracy: ,metrics.accuracy_score(prediction_gini,Y_validation)) #evaluation(Confusion Metrix) print( Confusion Metrix:\\n ,metrics.confusion_matrix(prediction_gini,Y_validation)) Menampilkan Pohon Data feature_cols=[ age , Time , Number_of_Warts , Type , Area ] dot_data = StringIO() export_graphviz(gini, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=[ Positive , negative ]) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png( gini.png ) Image(graph.create_png()) Menampilkan Hasil Prediksi Data Gini print( Hasil prediksi menngunakan gini ) #Membentuk kembali diperlukan untuk melakukan penggabungan pred_clf_df = pd.DataFrame(prediction.reshape(15,1)) #Ganti nama kolom untuk menunjukkan prediksi pred_clf_df.rename(columns={0: Prediction }, inplace=True) #membentuk kembali dataset uji X_validation_df = pd.DataFrame(X_validation.reshape(15 ,5)) #menggabungkan dua bingkai data panda di atas kolom untuk membuat dataset prediksi pred_outcome = pd.concat([X_validation_df, pred_clf_df], axis=1, join_axes=[X_validation_df.index]) pred_outcome.rename(columns = {0: age ,1: Time ,2: Number_of_Warts ,3: Type ,4: Area }, inplace=True) #cetak 10 baris prediksi akhir print((pred_outcome).head(15)) print ( \\n ) #mengevaluasi(Accuracy) print( Accuracy: ,metrics.accuracy_score(prediction_gini,Y_validation)) Kesimpulannya hasil perhitungan entropy dan gini hampir sama, namun tingkat akurasi perhitungan entropy lebih akurat dengan akurasi 0.93 sedangkan perhitungan gini akurasinya 0.86 Referensi https://medium.com/@mimubarok.mim/decision-tree-pohon-keputusan-6484ad30c289 https://medium.com/iykra/mengenal-decision-tree-dan-manfaatnya-b98cf3cf6a8d http://dyan123.blogspot.com/2012/03/pengertian-decision-tree.html","title":"Decision Tree"},{"location":"Tree/#decision-tree-classifier-gain","text":"\u200b Secara singkat bahwa Decision Tree merupakan salah satu metode klasifikasi pada Text Mining. Klasifikasi adalah proses menemukan kumpulan pola atau fungsi-fungsi yang mendeskripsikan dan memisahkan kelas data satu dengan lainnya, untuk dapat digunakan untuk memprediksi data yang belum memiliki kelas data tertentu (Jianwei Han, 2001). \u200b Decision Tree adalah sebuah struktur pohon, dimana setiap node pohon merepresentasikan atribut yang telah diuji, setiap cabang merupakan suatu pembagian hasil uji, dan node daun (leaf) merepresentasikan kelompok kelas tertentu. Level node teratas dari sebuah Decision Tree adalah node akar (root) yang biasanya berupa atribut yang paling memiliki pengaruh terbesar pada suatu kelas tertentu. Pada umumnya Decision Tree melakukan strategi pencarian secara top-down untuk solusinya. Pada proses mengklasifikasi data yang tidak diketahui, nilai atribut akan diuji dengan cara melacak jalur dari node akar (root) sampai node akhir (daun) dan kemudian akan diprediksi kelas yang dimiliki oleh suatu data baru tertentu. \u200b Kelebihan lain dari metode ini adalah mampu mengeliminasi perhitungan atau data-data yang kiranya tidak diperlukan. Sebab, sampel yang ada biasanya hanya diuji berdasarkan kriteria atau kelas tertentu saja. \u200b Meski memiliki banyak kelebihan, namun bukan berarti metode ini tidak memiliki kekurangan. Decision tree ini bisa terjadi overlap, terutama ketika kelas dan kriteria yang digunakan sangat banyak tentu saja dapat meningkatkan waktu pengambilan keputusan sesuai dengan jumlah memori yang dibutuhkan. \u200b Dalam hal akumulasi, decision tree juga seringkali mengalami kendala eror terutama dalam jumlah besar. Selain itu, terdapat pula kesulitan dalam mendesain decision tree yang optimal. Apalagi mengingat kualitas keputusan yang didapatkan dari metode decision tree sangat tergantung pada bagaimana pohon tersebut didesain. \u200b Terlepas dari kekurangan dan kelebihan dari decision tree , metode ini banyak digunakan lebih lanjut dalam berbagai pengolahan data. Mulai dari data mining dan juga machine learning . Dalam dunia kerja, decision tree sendiri sangat berguna untuk penilaian credit scoring. Jika anda pernah mengajukan kredit yang diproses secara instan, nah anda sudah mempunyai pengalaman dari decision tree .","title":"Decision Tree Classifier (Gain)"},{"location":"Tree/#struktur-dan-algoritma-dasar-pembelajaran-pohon-keputusan","text":"Pada pohon keputusan terdapat tiga jenis node , antara lain : 1. Akar Merupakan node teratas, pada node ini tidak ada input dan dapat tidak mempunyai output atau dapat mempunyai output lebih dari satu. 2. Internal node Merupakan node percabangan, pada node ini hanya terdapat satu input dan mempunyai output minimal dua. 3. Daun Merupakan node akhir atau terminal node , pada node ini hanya terdapat satu input dan tidak mempunyai output (simpul terminal). Kebanyakan algoritma untuk pembelajaran pohon keputusan adalah variasi dari algoritma intinya yang menggunakan pencarian rakus ( greedy ) dari atas ke bawah terhadap ruang kemungkinan pohon keputusan. Salah satu algoritmanya yaitu ID3. Langkah-langkah yang dilakukan yaitu, Tentukan atribut terbaik untuk root dari tree . Setiap instan dari atribut dievaluasi menggunakan sebuah tes statistik untuk menentukan seberapa bagus atribut tersebut mengklasifikasi sampel latihan. Turunan dari node root dibuat untuk setiap kemungkinan nilai dari atribut Sampel latihan kemudian diurutkan berdasarkan node turunan. Seluruh proses kemudian diulang menggunakan sampel latihan yang berhubungan dengan setiap node turunan untuk menentukan atribut terbaik untuk dites di pohon.","title":"Struktur dan Algoritma Dasar Pembelajaran Pohon Keputusan"},{"location":"Tree/#entropy-information-gain","text":"Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur \u201cseberapa informatifnya\u201d sebuah node (yang biasanya disebut seberapa baiknya). Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama. Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama. 0 Entropi(S) 1, jika jumlah contoh positif dan negatif dalam S tidak sama. Dimana: \u2022 S adalah himpunan (dataset) kasus \u2022 k adalah banyaknya partisi S \u2022 pj adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus. Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar.","title":"Entropy &amp; Information Gain"},{"location":"Tree/#implementasi-program-menggunakan-perhitungan-entropy","text":"Import Library import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn import model_selection from sklearn import metrics from sklearn.model_selection import train_test_split from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO import pydotplus from IPython.display import Image Load dataset #memuat file excel df=pd.read_excel( Cryotherapy.xlsx ) #menampilkan data print( Informasi Data\\n ) print( Jumlah Data : , len(df)) print ( Dimensi Data : ,df.shape) print ( Dataset : ) print(df.head()) print( \\n ) Memilah data jika data sudah sukses terimport, selanjutnya kita akan memilah data ke beberapa bagian seperti data yang akan d jadikan testing, training, dan yang akan dijadikan classnya. di sini saya menggunakan 15 data untuk testingnya dan data kolom terakhir sebagai classnya. berikut kodenya : #splitting dataset ke training dan testing train, test = train_test_split(df, test_size = 0.1,random_state=1234) #mencari hasil print(train.shape) print(test.shape) # Dataset validasi dataset array = df.values X = array[:,1:6] Y = array[:,6] validation_size = 15 seed = 7 X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed) Tampil Hasil Pemilahan Data #mencari hasil print(X_train.shape) print(Y_train.shape) print(X_validation.shape) print(Y_validation.shape) Prediksi Data Menggunakan Library scikit lear metode Decision Tree Entropy entropy= DecisionTreeClassifier(criterion= entropy ,random_state=1234) #learning entropy.fit(X_train,Y_train) #Prediksi prediction=entropy.predict(X_validation) #mengevaluasi(Accuracy) print( Accuracy: ,metrics.accuracy_score(prediction,Y_validation)) #evaluation(Confusion Metrix) print( Confusion Metrix:\\n ,metrics.confusion_matrix(prediction,Y_validation)) Menampilkan Pohon Data feature_cols=[ age , Time , Number_of_Warts , Type , Area ] dot_data = StringIO() export_graphviz(entropy, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=[ Positive , negative ]) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png( entropy.png ) Image(graph.create_png()) Menampilkan Hasil Prediksi Data Entropy print( Hasil prediksi menggunakan entropy ) #Membentuk kembali diperlukan untuk melakukan penggabungan pred_clf_df = pd.DataFrame(prediction.reshape(15,1)) #Ganti nama kolom untuk menunjukkan prediksi pred_clf_df.rename(columns={0: Prediction }, inplace=True) #membentuk kembali dataset uji X_validation_df = pd.DataFrame(X_validation.reshape(15 ,5)) #menggabungkan dua bingkai data panda di atas kolom untuk membuat dataset prediksi pred_outcome = pd.concat([X_validation_df, pred_clf_df], axis=1, join_axes=[X_validation_df.index]) pred_outcome.rename(columns = {0: age ,1: Time ,2: Number_of_Warts ,3: Type ,4: Area }, inplace=True) #cetak 10 baris prediksi akhir print((pred_outcome).head(15)) print ( \\n ) #mengevaluasi(Accuracy) print( Accuracy: ,metrics.accuracy_score(prediction,Y_validation))","title":"Implementasi Program Menggunakan Perhitungan Entropy"},{"location":"Tree/#prediksi-data-menggunakan-library-scikit-learn-metode-decision-tree-gini","text":"Kemudian kita coba menggunakan metode perhitungan gini. Adapun caranya sama seperti decision tree. yang membedakannya adalah criterionnya. kita tinggal mengubahnya menjadi gini. gini= DecisionTreeClassifier(criterion= gini ,random_state=1234) #learning gini.fit(X_train,Y_train) #Prediksi prediction_gini=gini.predict(X_validation) #mengevaluasi(Accuracy) print( Accuracy: ,metrics.accuracy_score(prediction_gini,Y_validation)) #evaluation(Confusion Metrix) print( Confusion Metrix:\\n ,metrics.confusion_matrix(prediction_gini,Y_validation)) Menampilkan Pohon Data feature_cols=[ age , Time , Number_of_Warts , Type , Area ] dot_data = StringIO() export_graphviz(gini, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=[ Positive , negative ]) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png( gini.png ) Image(graph.create_png()) Menampilkan Hasil Prediksi Data Gini print( Hasil prediksi menngunakan gini ) #Membentuk kembali diperlukan untuk melakukan penggabungan pred_clf_df = pd.DataFrame(prediction.reshape(15,1)) #Ganti nama kolom untuk menunjukkan prediksi pred_clf_df.rename(columns={0: Prediction }, inplace=True) #membentuk kembali dataset uji X_validation_df = pd.DataFrame(X_validation.reshape(15 ,5)) #menggabungkan dua bingkai data panda di atas kolom untuk membuat dataset prediksi pred_outcome = pd.concat([X_validation_df, pred_clf_df], axis=1, join_axes=[X_validation_df.index]) pred_outcome.rename(columns = {0: age ,1: Time ,2: Number_of_Warts ,3: Type ,4: Area }, inplace=True) #cetak 10 baris prediksi akhir print((pred_outcome).head(15)) print ( \\n ) #mengevaluasi(Accuracy) print( Accuracy: ,metrics.accuracy_score(prediction_gini,Y_validation)) Kesimpulannya hasil perhitungan entropy dan gini hampir sama, namun tingkat akurasi perhitungan entropy lebih akurat dengan akurasi 0.93 sedangkan perhitungan gini akurasinya 0.86 Referensi https://medium.com/@mimubarok.mim/decision-tree-pohon-keputusan-6484ad30c289 https://medium.com/iykra/mengenal-decision-tree-dan-manfaatnya-b98cf3cf6a8d http://dyan123.blogspot.com/2012/03/pengertian-decision-tree.html","title":"Prediksi Data Menggunakan Library scikit learn metode Decision Tree Gini"}]}